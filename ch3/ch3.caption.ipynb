{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870aa2db",
   "metadata": {},
   "source": [
    "# ch3.caption using RNN & EMBEDDING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe9f8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "annotation_dir = '/Users/jiae/Documents/tf2_real_project/ch3/Flickr8k_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7c674ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(os.path.join(annotation_dir,file_name),'rb') as file_handle:\n",
    "        file_lines = file_handle.read().splitlines()\n",
    "    return file_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "288548e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "1000\n",
      "40460\n"
     ]
    }
   ],
   "source": [
    "train_image_paths = read_file('Flickr_8k.trainImages.txt')\n",
    "test_image_paths = read_file('Flickr_8k.testImages.txt')\n",
    "captions = read_file('Flickr8k.token.txt')\n",
    "\n",
    "print(len(train_image_paths))\n",
    "print(len(test_image_paths))\n",
    "print(len(captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72173c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_str(file_name):\n",
    "    with open(os.path.join(annotation_dir,file_name),'r') as file_handle:\n",
    "        file_lines = file_handle.read().splitlines()\n",
    "    return file_lines\n",
    "train_image_paths_str = read_file_str('Flickr_8k.trainImages.txt')\n",
    "test_image_paths_str = read_file_str('Flickr_8k.testImages.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39b54bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "    image_caption_map = {}\n",
    "    unique_words1 = set()\n",
    "    max_words = 0\n",
    "    for caption in captions:\n",
    "        caption = caption.decode(\"utf-8\")\n",
    "        image_name = caption.split(\"#\")[0]\n",
    "        \n",
    "        image_caption = caption.split(\"#\")[1].split('\\t')[1]\n",
    "        \n",
    "        if image_name not in image_caption_map.keys():\n",
    "            image_caption_map[image_name] = [image_caption]\n",
    "        else:\n",
    "            image_caption_map[image_name].append(image_caption)\n",
    "        caption_words = image_caption.split()\n",
    "        max_words = max(max_words, len(caption_words))\n",
    "        [unique_words1.add(caption_word) for caption_word in caption_words]\n",
    "        \n",
    "        unique_words = list(unique_words1)\n",
    "        word_to_index_map = {}\n",
    "        index_to_word_map = {}\n",
    "        for index, unique_word in enumerate(unique_words):\n",
    "            word_to_index_map[unique_word] = index\n",
    "            index_to_word_map[index] = unique_words\n",
    "        #print(max_words)\n",
    "    return image_caption_map,max_words,unique_words,word_to_index_map,index_to_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8028ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1be6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        vgg_model = tf.keras.applications.vgg16.VGG16(\n",
    "    include_top=True, weights='imagenet')\n",
    "#         inputs = tf.keras.Input(shape=(224,224))\n",
    "#         outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
    "        self.model = tf.keras.Model(inputs=vgg_model.input, \n",
    "                                    outputs=vgg_model.get_layer('fc2').output)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_preprocess_image(image_path):\n",
    "        image_array = tf.keras.preprocessing.image.load_img(image_path,\n",
    "                                                           target_size=(224,224))\n",
    "        image_array = tf.keras.preprocessing.image.img_to_array(image_array)\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        image_array = tf.keras.applications.resnet50.preprocess_input(image_array)\n",
    "        return image_array\n",
    "    \n",
    "    def extract_feature_from_image_path(self,image_path):\n",
    "        image_array = self.load_preprocess_image(image_path)\n",
    "        features = self.model.predict(image_array)\n",
    "        return features.reshape((4096,1))\n",
    "    \n",
    "    def extract_feature_from_image_paths(self,work_dir,image_names):\n",
    "        features = {}\n",
    "        for image_name in image_names:\n",
    "            image_path = os.path.join(work_dir,image_name)\n",
    "            feature = self.extract_feature_from_image_path(image_path)\n",
    "            image_id = image_name.decode('utf-8')\n",
    "            features[image_id] = feature\n",
    "        return features\n",
    "    \n",
    "    def extract_features_and_save(self,work_dir,image_names,file_name):\n",
    "        features = self.extract_feature_from_image_paths(work_dir,image_names)\n",
    "        \n",
    "        with open(file_name,'wb') as p:\n",
    "            pickle.dump(features,p)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07b57dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = ImageModel()\n",
    "I.extract_features_and_save(b'Flicker8k_Dataset',train_image_paths,\n",
    "                           'train_image_features.p')\n",
    "I.extract_features_and_save(b'Flicker8k_Dataset',test_image_paths,\n",
    "                           'test_image_features.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d6c0474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_image_features.p', 'rb') as p:\n",
    "    sample = pickle.load(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f99da8",
   "metadata": {},
   "source": [
    "### dataset 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "20eaa917",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_caption_map,max_words,unique_words,word_to_index_map,index_to_word_map = get_vocab()\n",
    "vocabulary_size = len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "47b14626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_description(descriptions):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            desc = desc.split()\n",
    "            desc = [word.lower() for word in desc]\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            desc_list[i] = ' '.join(desc)\n",
    "    return descriptions\n",
    "\n",
    "clean_image_caption_map = clean_description(image_caption_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dcffcbca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d9/j4fzl6g10m97nw0c5yzh_bh40000gn/T/ipykernel_5336/2488202462.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_image_caption_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "set(clean_image_caption_map.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5c84178f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'spilt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d9/j4fzl6g10m97nw0c5yzh_bh40000gn/T/ipykernel_5336/2411269839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mall_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspilt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_desc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_image_caption_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/d9/j4fzl6g10m97nw0c5yzh_bh40000gn/T/ipykernel_5336/2411269839.py\u001b[0m in \u001b[0;36mto_vocabulary\u001b[0;34m(descriptions)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mall_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mall_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspilt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_desc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_image_caption_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/d9/j4fzl6g10m97nw0c5yzh_bh40000gn/T/ipykernel_5336/2411269839.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mall_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mall_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspilt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_desc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_image_caption_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'spilt'"
     ]
    }
   ],
   "source": [
    "def to_vocabulary(descriptions):\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.spilt()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "vocabulary = to_vocabulary(clean_image_caption_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a569dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = {}\n",
    "for img_idx in train_image_paths_str:\n",
    "    captions = image_caption_map[img_idx]\n",
    "    dic = {}\n",
    "    for i,caption in enumerate(captions):\n",
    "        words = caption.split()\n",
    "        y = [word_to_index_map[w] for w in words]\n",
    "        y_one_hot = tf.one_hot(y,vocabulary_size)\n",
    "#         dic[i] = y\n",
    "\n",
    "#     train_y[img_idx] = dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da313c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 9627), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fe6ec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 9627)\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(y_one_hot.shape)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eb92c343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 38, 128)           0         \n",
      "=================================================================\n",
      "Total params: 524,416\n",
      "Trainable params: 524,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_model = tf.keras.Sequential()\n",
    "image_model.add(tf.keras.layers.Dense(128, input_dim = 4096, activation = 'relu'))\n",
    "image_model.add(tf.keras.layers.RepeatVector(max_words))\n",
    "image_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b8647082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 38, 256)           2464512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 38, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 38, 128)           32896     \n",
      "=================================================================\n",
      "Total params: 3,022,720\n",
      "Trainable params: 3,022,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lang_model = tf.keras.Sequential()\n",
    "lang_model.add(tf.keras.layers.Embedding(vocabulary_size, 256, input_length=max_words))\n",
    "lang_model.add(tf.keras.layers.LSTM(256, return_sequences = True))\n",
    "lang_model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(128)))\n",
    "lang_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c8b389fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d9/j4fzl6g10m97nw0c5yzh_bh40000gn/T/ipykernel_5336/3431090268.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tf24/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2374\u001b[0m     \"\"\"\n\u001b[1;32m   2375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2377\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Concatenate([image_model, lang_model]))\n",
    "model.add(tf.keras.layers.LSTM(1000,return_sequences = False))\n",
    "model.add(tf.keras.layers.Dense(vocabulary_size,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.1),metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf24",
   "language": "python",
   "name": "tf24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
